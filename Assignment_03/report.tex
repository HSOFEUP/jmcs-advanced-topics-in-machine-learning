% #######################################
% ########### FILL THESE IN #############
% #######################################
\def\mytitle{ATML Assignment 03 Report}
\def\mykeywords{ATML, report, assignment 3, VAE, DAE, finetuning, transfer}
\def\myauthor{Lukas Zbinden}
\def\contact{lukas.zbinden@unifr.ch}
\def\mymodule{ATML, Spring 2018}
% #######################################
% #### YOU DON'T NEED TO TOUCH BELOW ####
% #######################################
\documentclass[10pt, a4paper]{article}
\usepackage[a4paper,outer=1.5cm,inner=1.5cm,top=1.75cm,bottom=1.5cm]{geometry}
\twocolumn
\usepackage{graphicx}
\graphicspath{{./images/}}
%colour our links, remove weird boxes
\usepackage[colorlinks,linkcolor={black},citecolor={blue!80!black},urlcolor={blue!80!black}]{hyperref}
%Stop indentation on new paragraphs
\usepackage[parfill]{parskip}
%% Arial-like font
\usepackage{lmodern}
\renewcommand*\familydefault{\sfdefault}
%Napier logo top right
\usepackage{watermark}
%Lorem Ipusm dolor please don't leave any in you final report ;)
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{listings}
%give us the Capital H that we all know and love
\usepackage{float}
%tone down the line spacing after section titles
\usepackage{titlesec}
%Cool maths printing
\usepackage{amsmath}
%PseudoCode
\usepackage{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amssymb}

\titlespacing{\subsection}{0pt}{\parskip}{-3pt}
\titlespacing{\subsubsection}{0pt}{\parskip}{-\parskip}
\titlespacing{\paragraph}{0pt}{\parskip}{\parskip}
\newcommand{\figuremacro}[5]{
    \begin{figure}[#1]
        \centering
        \includegraphics[width=#5\columnwidth]{#2}
        \caption[#3]{\textbf{#3}#4}
        \label{fig:#2}
    \end{figure}
}

\lstset{
	escapeinside={/*@}{@*/}, language=C++,
	basicstyle=\fontsize{8.5}{12}\selectfont,
	numbers=left,numbersep=2pt,xleftmargin=2pt,frame=tb,
    columns=fullflexible,showstringspaces=false,tabsize=4,
    keepspaces=true,showtabs=false,showspaces=false,
    backgroundcolor=\color{white}, morekeywords={inline,public,
    class,private,protected,struct},captionpos=t,lineskip=-0.4em,
	aboveskip=10pt, extendedchars=true, breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941}
}

%\thiswatermark{\centering %\put(336.5,-38.0){\includegraphics[scale=0.8]{logo}} }
\title{\mytitle}
\author{\myauthor\hspace{1em}\\\contact\\University of Bern\hspace{0.5em}-\hspace{0.5em}\mymodule}
\date{}
\hypersetup{pdfauthor=\myauthor,pdftitle=\mytitle,pdfkeywords=\mykeywords}
\sloppy
% #######################################
% ########### START FROM HERE ###########
% #######################################
\begin{document}
	\maketitle
    
	\section{DAE}
    \subsection{Evaluation of 3 denoising tasks}
    Table \ref{table:ta} depicts the best test accuracy for each of the three denoising autoencoders for the cases of fixed feature representation and finetuning. The numbers were achieved with 2 training epochs for the DAE and 7 epochs for the classifier (due to time constraints), respectively, and a transfer dataset size of 5000.
    TODO babble some more here...
    
    \begin{table}[h]
    \begin{center}
    \begin{tabular}{|l|c|c|}
    \hline
    DAE type & Fixed & Finetuning \\
    \hline\hline
    None & 68.6 & 85.7 \\
    Additive Gaussian & 67.8 & 87.9 \\
    Salt and Pepper & \textbf{73.0} & 88.3 \\
    Masking & 72.2 & \textbf{88.7} \\
    \hline
    \end{tabular}
    \end{center}
    \caption{Test accuracy for each type of DAE}
    \label{table:ta}
    \end{table}
    
    \subsection{Sample Reconstructions vs. Inputs}
    \subsubsection{Additive Gaussian noise}
    Figure \ref{fig:figag} depicts samples of additive Gaussian noise along with the reconstructions.
    
    \begin{figure}[h]
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/gaussian_add-encoded.png} 
    \caption{Encoder output}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/gaussian_add-decoded.png}
    \caption{Decoder output}
    \label{fig:subim2}
    \end{subfigure}
    \caption{Additive Gaussian noise samples and their respective reconstructions}
    \label{fig:figag}
    \end{figure}
    
    \subsubsection{Salt \& Pepper noise}
    Figure \ref{fig:figsp} depicts samples of Salt \& Pepper noise along with their reconstructions.
    
    \begin{figure}[h]
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_salt_pepper-encoded.png} 
    \caption{Encoder output}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_salt_pepper-decoded.png}
    \caption{Decoder output}
    \label{fig:subim2}
    \end{subfigure}
    \caption{Salt \& Pepper samples and their reconstructions}
    \label{fig:figsp}
    \end{figure}
    
    \subsubsection{Masking noise}
    Figure \ref{fig:figma} depicts samples of Masking noise along with their respective reconstructions.
    
    \begin{figure}[h]
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_masking-encoded.png} 
    \caption{Encoder output}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_masking-decoded.png}
    \caption{Decoder output}
    \label{fig:subim2}
    \end{subfigure}
    \caption{Masking samples and their reconstructions}
    \label{fig:figma}
    \end{figure}    
    
    \subsection{Which forms of noise are challenging or easy? Why?}
    Masking noise appears to be the most challenging as some of the images are perturbated to a degree to which the number becomes unrecognizable to the human eye. Also some of the respective reconstructions remain unclear what number they represent. So the decoder is challenged the most to reconstruct the image and in inpainting the missing regions, respectively.\newline
    Also challenging but to a lesser degree seems the Salt \& Pepper noise as it basically erases the original image pixel wise at some rate. However the reconstructions seem better recognizable. Lastly the additive Gaussian noise I consider the least distorting from considering the encoder output and the reconstructions, respectively. It is most likely the easiest for the DAE to reconstruct as the pixel values are not erased completely but only changed according to a normal distribution.\newline
    The more challenging a noise is to the DAE the more it is forced to learn the correct reconstruction of the original images and ultimately the better it's performance will be. This consideration is reflected in table \ref{table:ta} where the DAEs trained on masking and Salt \& Pepper noise, respectively, outperform the other types.
    
	
	\section{Transfer Learning}
	Some common formatting you may need uses these commands for \textbf{Bold Text}, \textit{Italics}, and \underline{underlined}.
	
	\subsection{Transfer Performance}
    TODO evaluate transfer performance of 3 different DAEs\\
    TODO Which type of noise results in the best features?\\
    TODO How do the features compare to features of a standard Autoencoder?
    
    \subsection{Impact of dataset size on performance of finetuning vs. fixed feature representation}
    What is the impact of finetuning vs. fixed feature representations and how does this change with the dataset size?
	
    \begin{table*}[h]
    \centering
    \begin{tabular}{|l||c||c|c|c|c||c|}
    \hline
    Transfer type & size & Gaussian & S \& P & masking & none & $\varnothing$  \\
    \hline\hline
    \multirow{3}{6em}{Fixed Feature} & \textbf{1000} & 43.3 & 26.9 & 32.0 & 39.2 & \textbf{35.6} \\
    & \textbf{2500} & 60.6 & 62.8 & 63.5 & 63.8 & \textbf{62.7} \\
    & \textbf{5000} & 67.8 & 73.0 & 72.2 & 68.6 & \textbf{70.4} \\
    \hline
    \multirow{3}{6em}{Finetuning} & \textbf{1000} & 32.6 & 35.7 & 41.5 & 41.4 & \textbf{37.8} \\
    & \textbf{2500} & 75.9 & 59.6 & 73.8 & 54.4 & \textbf{65.9} \\
    & \textbf{5000} & 87.9 & 88.3 & 88.7 & 85.7 & \textbf{87.7} \\
    \hline
    \end{tabular}
    \caption{Impact of dataset size on transfer performance}
    \label{table:imfidasi}
    \end{table*}    
    
    
	\subsection{Maths}
    Embedding Maths is Latex's bread and butter    
    
    {\centering \Large \(
        J = \begin{bmatrix}
            \frac{\delta e}{\delta \theta _0}
            \frac{\delta e}{\delta \theta _1}
            \frac{\delta e}{\delta \theta _2}
        \end{bmatrix}
        = e_{current} - e_{target} 
    \)\par}
	
	\subsection{Code Listing}
    You can load segments of code from a file, or embed them directly.
    

    
\subsection{PseudoCode}


	
\section{Conclusion}	
\bibliographystyle{ieeetr}
\bibliography{references}
		
\end{document}