% #######################################
% ########### FILL THESE IN #############
% #######################################
\def\mytitle{ATML Assignment 03 Report}
\def\mykeywords{ATML, report, assignment 3, VAE, DAE, finetuning, transfer}
\def\myauthor{Lukas Zbinden}
\def\contact{lukas.zbinden@unifr.ch}
\def\mymodule{ATML, Spring 2018}
% #######################################
% #### YOU DON'T NEED TO TOUCH BELOW ####
% #######################################
\documentclass[10pt, a4paper]{article}
\usepackage[a4paper,outer=1.5cm,inner=1.5cm,top=1.75cm,bottom=1.5cm]{geometry}
\twocolumn
\usepackage{graphicx}
\graphicspath{{./images/}}
%colour our links, remove weird boxes
\usepackage[colorlinks,linkcolor={black},citecolor={blue!80!black},urlcolor={blue!80!black}]{hyperref}
%Stop indentation on new paragraphs
\usepackage[parfill]{parskip}
%% Arial-like font
\usepackage{lmodern}
\renewcommand*\familydefault{\sfdefault}
%Napier logo top right
\usepackage{watermark}
%Lorem Ipusm dolor please don't leave any in you final report ;)
\usepackage{lipsum}
\usepackage{xcolor}
\usepackage{listings}
%give us the Capital H that we all know and love
\usepackage{float}
%tone down the line spacing after section titles
\usepackage{titlesec}
%Cool maths printing
\usepackage{amsmath}
%PseudoCode
\usepackage{algorithm2e}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{amssymb}

\titlespacing{\subsection}{0pt}{\parskip}{-3pt}
\titlespacing{\subsubsection}{0pt}{\parskip}{-\parskip}
\titlespacing{\paragraph}{0pt}{\parskip}{\parskip}
\newcommand{\figuremacro}[5]{
    \begin{figure}[#1]
        \centering
        \includegraphics[width=#5\columnwidth]{#2}
        \caption[#3]{\textbf{#3}#4}
        \label{fig:#2}
    \end{figure}
}

\lstset{
	escapeinside={/*@}{@*/}, language=C++,
	basicstyle=\fontsize{8.5}{12}\selectfont,
	numbers=left,numbersep=2pt,xleftmargin=2pt,frame=tb,
    columns=fullflexible,showstringspaces=false,tabsize=4,
    keepspaces=true,showtabs=false,showspaces=false,
    backgroundcolor=\color{white}, morekeywords={inline,public,
    class,private,protected,struct},captionpos=t,lineskip=-0.4em,
	aboveskip=10pt, extendedchars=true, breaklines=true,
	prebreak = \raisebox{0ex}[0ex][0ex]{\ensuremath{\hookleftarrow}},
	keywordstyle=\color[rgb]{0,0,1},
	commentstyle=\color[rgb]{0.133,0.545,0.133},
	stringstyle=\color[rgb]{0.627,0.126,0.941}
}

%\thiswatermark{\centering %\put(336.5,-38.0){\includegraphics[scale=0.8]{logo}} }
\title{\mytitle}
\author{\myauthor\hspace{1em}\\\contact\\University of Bern\hspace{0.5em}-\hspace{0.5em}\mymodule}
\date{}
\hypersetup{pdfauthor=\myauthor,pdftitle=\mytitle,pdfkeywords=\mykeywords}
\sloppy
% #######################################
% ########### START FROM HERE ###########
% #######################################
\begin{document}
	\maketitle
    
	\section{Denoising Autoencoder}
	We train and evaluate the performance of different denoising autoencoders (DAE) based on three noise data augmentation techniques, namely additive Gaussian noise, Salt \& Pepper noise and masking noise, respectively.
	
    \subsection{Evaluation of 3 denoising tasks}
    Table \ref{table:ta} depicts the best test accuracy for each of the three denoising autoencoders for the cases of fixed feature representation and finetuning. The numbers were achieved with 2 training epochs for the DAE and 7 epochs for the classifier (due to time constraints), respectively, and a transfer dataset size of 5000, on which the classifier was trained. The finetuned model trained on masked image data outperforms the others. However for the fixed DAEs the Salt \& Pepper noise originates the most challening training data such that 'its' DAE performs best. In all but one case the DAE outperform the standard autoencoder, based on which we can conclude that the noise data augmentation technique is effective in creating a more performant model.
    
    \begin{table}[h]
    \begin{center}
    \begin{tabular}{|l|c|c|}
    \hline
    DAE type & Fixed & Finetuning \\
    \hline\hline
    None & 68.6 & 85.7 \\
    Additive Gaussian & 67.8 & 87.9 \\
    Salt \& Pepper & \textbf{73.0} & 88.3 \\
    Masking & 72.2 & \textbf{88.7} \\
    \hline
    \end{tabular}
    \end{center}
    \caption{Test accuracy for each type of DAE}
    \label{table:ta}
    \end{table}
    
    \subsection{Sample Reconstructions vs. Inputs}
    Following we show sample noisy inputs along with their reconstructions as they are output by the DAE's decoder.
    
    \subsubsection{Additive Gaussian noise}
    Figure \ref{fig:figag} depicts samples of additive Gaussian noise along with the reconstructions.
    
    \begin{figure}[h]
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/gaussian_add-encoded.png} 
    \caption{Encoder output}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/gaussian_add-decoded.png}
    \caption{Decoder output}
    \label{fig:subim2}
    \end{subfigure}
    \caption{Additive Gaussian noise samples and their respective reconstructions}
    \label{fig:figag}
    \end{figure}
    
    \subsubsection{Salt \& Pepper noise}
    Figure \ref{fig:figsp} depicts samples of Salt \& Pepper noise along with their reconstructions.
    
    \begin{figure}[h]
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_salt_pepper-encoded.png} 
    \caption{Encoder output}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_salt_pepper-decoded.png}
    \caption{Decoder output}
    \label{fig:subim2}
    \end{subfigure}
    \caption{Salt \& Pepper samples and their reconstructions}
    \label{fig:figsp}
    \end{figure}
    
    \subsubsection{Masking noise}
    Figure \ref{fig:figma} depicts samples of Masking noise along with their respective reconstructions.
    
    \begin{figure}[h]
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_masking-encoded.png} 
    \caption{Encoder output}
    \label{fig:subim1}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
    \includegraphics[width=1.0\linewidth]{images/noise_masking-decoded.png}
    \caption{Decoder output}
    \label{fig:subim2}
    \end{subfigure}
    \caption{Masking samples and their reconstructions}
    \label{fig:figma}
    \end{figure}    
    
    \subsection{Which forms of noise are challenging or easy? Why?}
    Masking noise appears to be the most challenging as some of the images are perturbated to a degree to which the number becomes unrecognizable to the human eye. Also some of the respective reconstructions remain unclear what number they represent. So the decoder is challenged the most to reconstruct the image and in inpainting the missing regions, respectively.\newline
    Also challenging but to a lesser degree seems the Salt \& Pepper noise as it basically erases the original image pixel wise at some rate. However the reconstructions seem better recognizable. Lastly the additive Gaussian noise I consider the least distorting from considering the encoder output and the reconstructions, respectively. It is most likely the easiest for the DAE to reconstruct as the pixel values are not erased completely but only changed according to a normal distribution.\newline
    The more challenging a noise is to the DAE the more it is forced to learn the correct reconstruction of the original images and ultimately the better it's performance will be. This consideration is reflected in table \ref{table:ta} where the DAEs trained on masking and Salt \& Pepper noise, respectively, outperform the other types.
    
	
	\section{Transfer Learning}
	Here we evaluate the quality of learnt representations in transfer learning experiments for classification based on the MNIST dataset.
	
	\begin{table}[b]
    \centering
    \begin{tabular}{|l||c||c|c|c|c||c|}
    \hline
    Transfer type & size & Gaussian & S \& P & masking & none & $\varnothing$  \\
    \hline\hline
    \multirow{3}{6em}{Fixed Feature} & \textbf{1000} & 43.3 & 26.9 & 32.0 & 39.2 & \textbf{35.6} \\
    & \textbf{2500} & 60.6 & 62.8 & 63.5 & 63.8 & \textbf{62.7} \\
    & \textbf{5000} & 67.8 & 73.0 & 72.2 & 68.6 & \textbf{70.4} \\
    \hline
    \multirow{3}{6em}{Finetuning} & \textbf{1000} & 32.6 & 35.7 & 41.5 & 41.4 & \textbf{37.8} \\
    & \textbf{2500} & 75.9 & 59.6 & 73.8 & 54.4 & \textbf{65.9} \\
    & \textbf{5000} & 87.9 & 88.3 & 88.7 & 85.7 & \textbf{87.7} \\
    \hline
    \end{tabular}
    \caption{Transfer performance of four DAEs for different transfer dataset sizes and two transfer types. The last column shows the average test accuracy across the four DAEs.}
    \label{table:imfidasi}
    \end{table}  
	
	\subsection{Transfer Performance}
	Table \ref{table:imfidasi} shows the transfer performance for two transfer types: 'fixed feature' in which the trained encoder is fixed for the training of the classifier and 'finetuning' in which the trained encoder is continued to be trained when training the classifier (i.e. the new task). For each type, the performances are collected for different transfer dataset sizes, i.e. the dataset used to train the classifier (and in case of 'finetuning' als the encoder), and finally for each of the four DAEs. The last column then shows the average test accuracy across the latter. It stands out that the average performance in the finetuning case is much better than in the fixed case. 
	
	\subsubsection{Type of noise to yield the best features}
	The best features are achieved by the masking DAE and finetuning transfer with a 88.7\% accuracy. Yet all noise types achieve comparable best results for the largest transfer dataset size with 'finetuning' whereas they differ more substantially as the dataset decreases. 
	
	\subsubsection{Comparing the features to features of a standard autoencoder}
    The performance of the standard AE improves with increasing dataset size in a similar fashion as the DAEs do. The standard AE delivers in no case the very best results, however some numbers are very close to the highest. In general the DAEs, in particular those based on S \& P and masking noises, outperform the standard AE especially as the dataset size increases.
    
    \subsection{Impact of dataset size on performance of finetuning vs. fixed feature representation}
    Figure \ref{fig:diagfine} depicts the impact of the transfer dataset size (i.e. on which the classifer is learnt) on the performance of both finetuning and fixed feature representation transfer learning. It shows that finetuning outperforms the fixed variant with respect to all sizes and further continues to deliver results in an almost linear way whereas the fixed representation drops its performance when the dataset gets much bigger with a size of 5000. 
    
    \begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{images/finetune_diag.png}
    \caption{Impact of dataset size on transfer learning type}
    \label{fig:diagfine}
    \end{figure}
	
	\section{Conclusion}
	The experiments show that using noise as a form of data augmentation yields notable improvements over the standard dataset as the autoencoders are forced to learn to process harder training examples and thus develop more elaborate feature representations. Further, the experiments demonstrate that transfer learning is a complementary and also effective technique to improve the performance of autoencoders especially in the case of finetuning. Transfer learning is complementary because in the combination of the two techniques lies the best performance as seen in the case of masking and S \& P noise, respectively.
	
\end{document}